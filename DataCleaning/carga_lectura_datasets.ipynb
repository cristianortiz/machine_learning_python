{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga de datos a traves de la funcion read_csv\n",
    "import pandas as pd\n",
    "import os\n",
    "mainpath = \"/Users/cristian/Google Drive/JupyterNotebooks/machine_learning/datasets\"\n",
    "filename = \"titanic/titanic3.csv\"\n",
    "fullpath = os.path.join(mainpath,filename)\n",
    "\n",
    "#guarda los datos del archivo en un dataframe\n",
    "data = pd.read_csv(fullpath)\n",
    "#muestra los 5 primeros registros o filas del dataset\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parametros importantes de la funcion read.csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "-dtype puede definir columnas con un tipo de dato especifico\n",
    "-header define que fila es cabecera y names define cabeceras si el dataset no trae\n",
    "-skiprows  permite saltarse filas para leer, si vale 12, se empezaria a leer desde la fila 13\n",
    "-index_col permite etiquetar filas\n",
    "-skip_blank_lines, se salta espacios en blanco\n",
    "-na_filter = True elimina filas con valores nulos o desconocidos del dataset, cuidado con este...\"\"\"\n",
    "#read.csv(filepath=\"/Users/cristian/Google Drive/Jupyter notebooks/machine_learning/datasets/titanic/titanic3.csv\",\n",
    "#        sep = \",\",dtype={\"a\":np.float64, \"b\":np.int32},header=0,names={\"ingresos\",\"edad\"},skiprows=None,index_col=None,\n",
    "#        skip_blank_lines=False,na_filter=False) \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de cambiar las cabeceras de un data set, caso frecuente de que se entreguen por separado el dataset y sus cabeceras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargo un dataset que en txt, pero separado por comas, con sus cabeceras originales\n",
    "data2  = pd.read_csv(mainpath + \"/\" + \"customer-churn-model/Customer Churn Model.txt\")\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desde el dataframe podemos ver los nombres de las columnas\n",
    "data2.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desde otro archivo cargo las cabeceras y las transformo en una lista\n",
    "data_cols = pd.read_csv(mainpath+ \"/\" +\"customer-churn-model/Customer Churn Columns.csv\")\n",
    "data_col_list = data_cols[\"Column_Names\"].tolist()\n",
    "data_col_list\n",
    "#ahora cargo de nuevo el dataset, pero le indico que las cabeceras de las columnas seran las de data_col_list\n",
    "data2 = pd.read_csv(mainpath + \"/\" + \"customer-churn-model/Customer Churn Model.txt\", header = None, names =data_col_list)\n",
    "data2.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carga de datos con la funcion open, para datasets muy grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abro el dataset con la funcion open en modo de solo lectura 'r'\n",
    "data3 = open(mainpath + \"/\"+\"customer-churn-model/Customer Churn Model.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lee la primera linea (la de cabeceras), quita los espacios en blanco al inicio y al final, strip, y divide el texto con comas, \n",
    "#split, formando un array\n",
    "cols = data3.readline().strip().split(\",\")\n",
    "#obtengo el numero de columnas..21\n",
    "n_cols = len(cols)\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea un diccionario con el array anterior mas un array vacio por cada cabecera, como clave del array, estilo JSON\n",
    "counter = 0\n",
    "main_dict = {}\n",
    "for col in cols:\n",
    "    main_dict[col] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rellenar el diccionario leyendo cada linea del dataset (line), esquema de lectura y llenado de matriz NxN\n",
    "for line in data3:\n",
    "    #cada valor se obtendra de cada linea del dataset quitando espacios vacios y dividido por comas, como antes\n",
    "    values = line.strip().split(\",\") \n",
    "    #cada [] del diccionario sera un array con los valores leidos y procesados desde el dataset\n",
    "    for i in range(len(cols)):\n",
    "        main_dict[cols[i]].append(values[i])\n",
    "    counter +=1\n",
    "print(\"El dataset tiene %d filas y %d columnas\" %(counter,n_cols))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creamos un dataframe de panda y el resultado es el mismo que con la funcion read_csv, recordar que esto es para grandes\n",
    "#datasets que podrian ser cargados en partes, o incluso en distintas maquinas y hacer la carga manual de datos\n",
    "df3 = pd.DataFrame(main_dict)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lectura y escritura de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile =  mainpath + \"/\" +\"customer-churn-model/Customer Churn Model.txt\"\n",
    "outfile = mainpath + \"/\" +\"customer-churn-model/Tab Customer Churn Model.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(infile,\"r\") as infile1:\n",
    "    with open(outfile,\"w\") as outfile1:\n",
    "        for line in infile1:\n",
    "            fields = line.strip().split(\",\")\n",
    "            outfile1.write(\"\\t\".join(fields))\n",
    "            outfile1.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(outfile,sep=\"\\t\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer datos desde una URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "medals_url = \"http://winterolympicsmedals.com/medals.csv\"\n",
    "medals_data = pd.read_csv(medals_url)\n",
    "medals_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incompleto, ver como trabajar con esta librria en conjunto conlas funciones de la libreria csv\n",
    "import urllib3\n",
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET',medals_url)\n",
    "#r.status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archivos XLS y XLSX (Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = \"/Users/cristian/Google Drive/JupyterNotebooks/machine_learning/datasets\"\n",
    "filename = \"titanic/titanic3.xls\"\n",
    "titanic2 = pd.read_excel(mainpath+\"/\"+filename,\"titanic3\")\n",
    "titanic2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo un archivo .csv a partir de un dataframe, y tambien se puede crear un excel\n",
    "titanic2.to_csv(mainpath + \"/titanic/titanic_custom2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.to_excel(mainpath + \"/titanic/titanic_custom3.xls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
